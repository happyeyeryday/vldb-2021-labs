# VLDB2021（第二次大作业）\

---

**ECNU2025数据管理系统project2-2**

**VLDBSummerSchool2021Labs**

**指导老师**：周烜

**队伍成员**：10234304416 朱昭荣/10235304430司南

---

## 理论上知识准备

实验过程通过相关 lab 代码，从存储、日志事务引擎出发逐步完善，支持 SQL 引擎，最终实现一个完整的支持分布式事务的分布式数据库内核。需要对于数据库理论、分布式系统有相关基础知识了解，在每个 lab 对应 参考文档中，也有对应参考资料链接。lab 设计和代码实现参考 TiDB [架构](https://docs.pingcap.com/zh/tidb/stable/tidb-architecture/)，可通过 [TiDB Book](https://book.tidb.io/session1/chapter1/tidb-architecture.html) 了解相关内容。



## 机器资源

实验过程需要在本地开发测试、调试，推荐本地开发机器有较好的处理器、内存、硬盘资源配置：

- 处理器资源配置没有特殊要求，更好的配置利于更快的运行测试程序
- 运行部分实验需要约 8GB 内存，如果运行测试时遇到内存不足的相关错误，可以尝试调低 环境变量，详见*本地编码和测试*一节`GOGC`
- 硬盘推荐使用固态硬盘，预留至少 50GB 空间
- 

## 实验准备

vldb summer school 2021 的所有实验相关组件均使用 [`golang`](https://go.dev/) 实现，需要在开发机配置 golang 相关开发环境。

### 安装 golang

参考 golang [get started](https://go.dev/learn/) 根据开发机平台进行安装，推荐安装 v1.16 golang 版本。 安装完成后可以本地编译 golang 代码生成可执行程序，例如

验证方法：

```
bash> go version
go version go1.16.4 linux/amd64
```

注意是1.16.4版本！

### 实验测试

原本实验有线上提交测试，以及上传自动评分。

那我们就从git上clone仓库后进行本地测试和debug



## 最终测试结果

经过团队成员通力合作和不断讨论，通过了全部测试案例（**test 测试案例通过情况占40%**）！

所有测试结果，可见[img文件夹](https://gitee.com/ruochen1238/vldb-2021-labs/tree/master/img)，后续具体实验部分也会贴出测试结果。



## 实验结构 

这个实验的分布式数据库中有几个模块

-  TinyKV：系统的存储引擎； 
- TinyScheduler：用于管理和调度TinyKV集群；
-  TinySQL：TinyKV引擎的SQL层。 

我们需要完成四个lab，它们分别对应以下几个功能的补全：

1. lab1:在TinyKV中实现存储和日志层;
2. lab2:在TinyKV中实现事务层; 
3. lab3:实现Percolator协议; 
4. lab4:实现SQL执行层; 
5. lab4-A:实现SQL协议; 
6. lab4-B:实现更新执行器; 
7. lab4-C:实现选择和投影执行器。

每个 lab 都要写一些代码，然后跑测试看是否通过。按顺序做就行，前面的 lab 是后面的基础。

## 项目总览

![image-20250609212405389](C:\Users\tel13803506661\AppData\Roaming\Typora\typora-user-images\image-20250609212405389.png)

```
tinykv/                                    # TinyKV 分布式键值存储项目根目录
├── kv/                                    # TinyKV 核心实现模块
│   ├── raftstore/                        # Lab1: Raft 存储层实现
│   │   ├── bootstrap.go                  # 集群启动和初始化
│   │   ├── fsm.go                        # 有限状态机实现
│   │   ├── msg.go                        # 消息定义和处理
│   │   ├── peer.go                       # Peer 节点核心逻辑
│   │   ├── peer_msg_handler.go           # Peer 消息处理器
│   │   ├── peer_storage.go               # Peer 持久化存储
│   │   ├── raft_worker.go                # Raft 工作线程池
│   │   ├── router.go                     # 消息路由器
│   │   ├── runner.go                     # 后台任务运行器
│   │   ├── snap/                         # 快照管理
│   │   │   ├── snap.go                   # 快照操作接口
│   │   │   └── snap_manager.go           # 快照管理器
│   │   ├── store.go                      # Store 存储抽象
│   │   ├── store_worker.go               # Store 工作线程
│   │   └── util.go                       # 工具函数
│   ├── storage/                          # Lab1: 存储引擎层
│   │   ├── standalone_storage/           # 单机存储实现
│   │   │   ├── standalone_storage.go     # 单机存储主体
│   │   │   └── standalone_reader.go      # 单机存储读取器
│   │   └── raft_storage/                 # Raft 分布式存储
│   │       ├── raft_storage.go           # Raft 存储实现
│   │       └── raft_server.go            # Raft 存储服务器
│   ├── transaction/                      # Lab2: 分布式事务层
│   │   ├── commands/                     # 事务命令实现
│   │   │   ├── checkTxn.go               # 事务状态检查命令
│   │   │   ├── commit.go                 # 事务提交命令
│   │   │   ├── get.go                    # 事务读取命令
│   │   │   ├── prewrite.go               # 事务预写命令
│   │   │   ├── resolve.go                # 锁解析命令
│   │   │   └── rollback.go               # 事务回滚命令
│   │   ├── mvcc/                         # MVCC 多版本并发控制
│   │   │   ├── lock.go                   # 锁结构定义
│   │   │   ├── scanner.go                # MVCC 扫描器
│   │   │   ├── transaction.go            # MVCC 事务实现
│   │   │   └── write.go                  # 写记录定义
│   │   ├── latches/                      # 内存锁存器
│   │   │   └── latches.go                # 本地锁管理
│   │   └── server.go                     # 事务服务器
│   ├── server/                           # Lab3: 上层服务接口
│   │   ├── server.go                     # 服务器主体框架
│   │   ├── raw_api.go                    # Raw API 实现
│   │   ├── server_stand_alone.go         # 单机模式服务器
│   │   └── server_raft.go                # Raft 模式服务器
│   ├── coprocessor/                      # Lab4: 协处理器
│   │   ├── coprocessor.go                # 协处理器主体
│   │   └── builder.go                    # 查询构建器
│   └── util/                             # 通用工具模块
│       ├── engine_util/                  # 存储引擎工具
│       │   ├── engines.go                # 引擎接口定义
│       │   ├── cf_iterator.go            # 列族迭代器
│       │   └── util.go                   # 存储工具函数
│       └── worker/                       # 工作线程池
│           ├── worker.go                 # 工作线程抽象
│           └── task.go                   # 任务定义
├── raft/                                 # Lab1: Raft 共识算法核心
│   ├── raft.go                           # Raft 状态机主体
│   ├── log.go                            # Raft 日志管理
│   ├── rawnode.go                        # RawNode 接口实现
│   ├── storage.go                        # Raft 存储接口
│   ├── util.go                           # Raft 工具函数
│   └── doc.go                            # Raft 文档说明
├── scheduler/                            # Lab3: PD 调度器模块
│   ├── server/                           # 调度服务器
│   │   ├── cluster.go                    # 集群管理
│   │   ├── config.go                     # 配置管理
│   │   ├── coordinator.go                # 调度协调器
│   │   ├── id.go                         # ID 分配器
│   │   ├── schedulers/                   # 具体调度器
│   │   │   ├── balance_region.go         # 区域负载均衡
│   │   │   └── scheduler.go              # 调度器接口
│   │   ├── server.go                     # 调度服务器主体
│   │   └── util.go                       # 调度工具函数
│   └── client/                           # 调度客户端
│       ├── client.go                     # PD 客户端实现
│       └── option.go                     # 客户端选项
├── proto/                                # Protocol Buffers 定义
│   ├── pkg/                              # 生成的 Go 代码
│   │   ├── coprocessor/                  # 协处理器 protobuf
│   │   ├── errorpb/                      # 错误消息 protobuf
│   │   ├── kvrpcpb/                      # KV RPC protobuf
│   │   ├── metapb/                       # 元数据 protobuf
│   │   ├── raft_cmdpb/                   # Raft 命令 protobuf
│   │   ├── raft_serverpb/                # Raft 服务器 protobuf
│   │   └── schedulerpb/                  # 调度器 protobuf
│   └── proto/                            # 原始 .proto 文件
│       ├── coprocessor.proto             # 协处理器协议定义
│       ├── errorpb.proto                 # 错误协议定义
│       ├── kvrpcpb.proto                 # KV RPC 协议定义
│       ├── metapb.proto                  # 元数据协议定义
│       ├── raft_cmdpb.proto              # Raft 命令协议
│       ├── raft_serverpb.proto           # Raft 服务器协议
│       └── schedulerpb.proto             # 调度器协议定义
├── log/                                  # 日志处理模块
│   ├── log.go                            # 日志接口和实现
│   └── zap.go                            # Zap 日志器封装
├── doc/                                  # 实验文档目录
│   ├── lab1.md                           # Lab1: Raft 和存储实验指南
│   ├── lab2.md                           # Lab2: 分布式事务实验指南
│   ├── lab3.md                           # Lab3: 多 Raft 组实验指南
│   ├── lab4.md                           # Lab4: 事务 API 实验指南
│   └── imgs/                             # 文档配图目录
│       ├── architecture.png              # 架构图
│       ├── raft-overview.png             # Raft 算法图
│       └── transaction-overview.png       # 事务流程图
├── tests/                                # 测试文件目录
│   ├── integrations/                     # 集成测试
│   │   ├── raft/                         # Raft 集成测试
│   │   ├── transaction/                  # 事务集成测试
│   │   └── server/                       # 服务器集成测试
│   └── fixtures/                         # 测试数据
├── bin/                                  # 编译输出目录
│   ├── tinykv-server                     # TinyKV 服务器可执行文件
│   └── tinyscheduler                     # TinyScheduler 可执行文件
├── scripts/                              # 脚本工具目录
│   ├── ci/                               # 持续集成脚本
│   ├── dev/                              # 开发辅助脚本
│   └── deploy/                           # 部署脚本
├── vendor/                               # Go 依赖包（可选）
├── .gitignore                            # Git 忽略文件配置
├── .golangci.yml                         # Go 代码检查配置
├── Makefile                              # 构建和测试脚本
├── README.md                             # 项目说明文档
├── go.mod                                # Go 模块定义
├── go.sum                                # Go 模块校验和
└── LICENSE                               # 开源许可证
```

简而言之

- **TiKV**是一个分布式的存储引擎，是真正的服务实现端
- TiDB是一个sql层，本身并不存储数据，只是将SQL查询解析为操作，将实际的数据读取请求转发给底层的存储层TiKV
- **PD**是整个TiDB集群的元信息管理模块，在TiDB和TiKV之间调度数据的分布和流量。

每个目录都有明确的职责，比如 `raft/` 目录就是 Raft 算法的核心实现，`kv/transaction/` 就是事务相关的代码。这样的设计让代码结构很清晰，也方便我们按模块去理解和实现。

总的来说，这个实验设计得还是很用心的，既能学到分布式系统的核心概念，又能体验到真实的工程实践。虽然过程中会遇到一些挑战，但最终完成后还是很有成就感的。（撒花）

## LAB 1 The Storage And Log Layer.

---

可视化理解raft网站：[Raft 分布式共识算法动画演示](http://www.kailing.pub/raft/index.html)

Raft 核心机制详解

### Leader 选举机制

**选举触发条件**： 当 Follower 在 election timeout 时间内没有收到 Leader 的心跳信号时，就会转变为 Candidate 并发起新一轮选举。

**选举流程**：

1. **增加任期号**：Candidate 将自己的 Term（任期号）加 1

2. **发送投票请求**：向集群中所有其他节点发送 RequestVote 消息

3. 等待投票结果

   ：

   - 收到大多数节点投票 → 成为 Leader
   - 收到来自更高 Term 的消息 → 退回 Follower
   - 选票分裂无法获得多数票 → 等待超时重新选举

**选票分裂处理**： 当多个 Candidate 同时发起选举时，可能导致选票分散，无人获得多数票。此时各 Candidate 会等待随机的超时时间后重新发起选举，避免持续冲突。

#### 日志复制机制

**日志结构设计**： 每条日志条目包含三个关键信息：

- **term**：日志创建时的任期号，用于区分不同选举周期
- **index**：日志在序列中的位置，确保有序性
- **command**：客户端提交的具体操作内容

**(term, index) 元组是日志条目的唯一标识符**

**日志同步流程**：

1. **接收请求**：Leader 收到客户端请求，先写入自己的日志
2. **并行复制**：通过 AppendEntries 消息将日志同步到所有 Follower
3. **等待确认**：Leader 等待大多数节点的复制确认
4. **提交应用**：收到多数确认后，Leader 提交日志并应用到状态机
5. **通知提交**：Leader 在后续的 AppendEntries 中通知 Follower 提交相应日志

##### 一致性保证机制

**日志一致性检查**： Leader 通过比较 (term, index) 来检查 Follower 的日志是否与自己一致：

- 如果 Follower 的日志存在冲突，Leader 会强制回滚
- 回滚过程持续到找到双方都认可的匹配点
- 然后从匹配点开始重新同步后续日志

**提交规则**：

- **安全提交**：只有被大多数节点成功存储的日志才能被标记为已提交
- **顺序应用**：已提交的日志按照 index 顺序应用到状态机
- **一致性保证**：这确保了所有节点的状态机状态保持一致

##### 安全性原则

**Leader 完整性**：新选出的 Leader 必须包含所有已提交的日志条目，这通过投票阶段的日志比较来保证。

**状态机安全性**：如果某个节点已经将索引为 i 的日志应用到状态机，那么其他节点在相同索引位置不会应用不同的日志。

**追加性**：Raft 保证日志的单调性，已存在的日志条目不会被修改，只能追加新条目。

这些机制共同确保了 Raft 算法在分布式环境下的强一致性和高可用性，是 TinyKV 实现可靠分布式存储的理论基础。

TinyKV 项目是对 TiDB 生态系统的教学简化版本，完整复刻了现代分布式数据库的核心架构

Lab1 是整个项目的基础，我们要在这里实现存储层和共识算法。

其实就是要解决两个问题：怎么可靠地存储数据，怎么让多个节点对数据达成一致。

简单来说，TinyKV就是个分布式的键值存储系统，类似 Redis 但是支持分布式。它用 Raft 算法来保证多个节点的数据一致性，用类似 RocksDB 的存储引擎来持久化数据。

这个实验分成几个部分：

从项目结构来看，Lab1 涉及的核心文件主要集中在 `raft/` 目录和 `kv/storage/` 目录。必须先了解一下这些文件的作用（**已有代码结构的理解和阅读**）：

在 `raft/` 目录下：

- `raft.go` 是 Raft 算法的核心实现，包含了状态机的主要逻辑
- `log.go` 负责管理 Raft 日志，处理日志的存储、检索和复制
- `rawnode.go` 提供了上层应用使用 Raft 的接口封装

在 `kv/storage/` 目录下：

- `standalone_storage/` 实现了单机版的存储引擎
- 这部分相对简单，主要是对底层存储接口的封装

### RaftStore 模块详解

RaftStore 是 TinyKV 中使用 Raft 共识算法进行日志复制的核心模块。它负责管理分布式节点间的数据一致性，确保系统的可靠性。

##### RawNode - Raft 的高层封装

RawNode 是对 Raft 实例的封装，提供了更易用的接口供上层调用。它的主要特点是：

**职责分离**：RawNode 本身不负责消息发送和日志持久化，这些操作通过 Ready 结构体交给上层处理。

**核心接口**：

- `Step()` - 接收外部事件（客户端请求、Raft 消息、定时器事件）
- `Ready()` - 返回需要处理的状态变化
- `Advance()` - 通知 RawNode 继续下一轮处理

**工作流程**：

1. 上层通过 Step 传递各种事件到 RawNode
2. RawNode 驱动 Raft 实例生成 Ready 结构体
3. 上层处理 Ready 中的内容（发送消息、持久化日志等）
4. 完成后调用 Advance 进入下一轮

##### Region - 数据分片机制

TinyKV 采用 Key-Value 存储模型，其核心设计思想是：

**数据组织**：

- 整个系统是一个巨大的有序 Map
- 按照 Key 的二进制顺序排列
- 支持范围查询和顺序遍历

**Region 划分**：

- 将整个 Key 空间分成多个连续的段
- 每个 Region 用 [StartKey, EndKey) 区间表示
- 以 Region 为单位进行数据分布和复制

**副本管理**：

- 每个 Region 有多个 Replica（副本）
- 多个 Replica 分布在不同节点上
- 一个 Region 的所有 Replica 构成一个 Raft Group

##### RaftStore 工作流程

整个 RaftStore 的工作流程可以分为几个关键阶段：

**输入处理**： 所有输入事件进入 FIFO 队列，包括：

- **Request（客户端请求）**：读写操作，写操作会转化为日志条目
- **Message（Raft 消息）**：节点间的选举、日志复制等消息
- **Tick（定时器事件）**：选举超时、心跳等定时触发的事件

**Raft 状态机处理**： 状态机从队列中取出事件进行处理：

- **选举处理**：超时时发起选举
- **日志复制**：将客户端请求转化为日志并复制到其他节点
- **一致性维护**：根据 Raft 消息更新状态或回应

**输出结果**： 状态机可能产生以下输出：

- **发送消息**：向其他节点发送复制请求、投票等
- **追加日志**：将操作写入本地日志
- **提交日志**：标记被大多数节点确认的日志为已提交

**日志应用**：

- 已提交的日志进入应用队列
- 按顺序应用到状态机（更新存储数据）
- 向客户端返回响应结果

##### 关键设计原则

**异步处理**：通过 FIFO 队列和 Ready 机制实现异步处理，提高系统性能。

**职责分离**：RawNode 专注于 Raft 逻辑，上层负责具体的 I/O 操作。

**批量操作**：通过 Ready 结构体批量返回需要处理的操作，减少系统调用开销。

这种设计让 TinyKV 既保证了数据的强一致性，又具备了良好的性能和可扩展性。

### Badger 数据库简介

Badger 是一个用 Go 语言编写的高性能嵌入式键值数据库，在 TinyKV 项目中作为底层存储引擎使用。

#### 基本特点

**语言原生**：完全用 Go 语言实现，与 TinyKV 项目无缝集成，避免了 CGO 调用的开销。

**嵌入式设计**：直接嵌入到应用程序中，不需要单独的数据库服务进程，简化了部署和管理。

**高性能**：采用 LSM-Tree（Log-Structured Merge Tree）存储结构，对写入操作进行了优化，适合写多读少的场景。

#### 核心架构

**LSM-Tree 结构**：

- 数据先写入内存中的 MemTable
- MemTable 满了后刷写到磁盘形成 SSTable 文件
- 多个 SSTable 文件会定期进行合并压缩
- 这种设计让写入操作非常快速

**事务支持**：

- 支持 ACID 事务特性
- 提供读写事务和只读事务两种模式
- 通过 MVCC（多版本并发控制）实现并发访问

#### 在 TinyKV 中的作用

**存储抽象**： Badger 在 TinyKV 中被封装成统一的存储接口，上层代码不需要关心具体的存储实现细节。

**列族模拟**： 虽然 Badger 原生不支持列族（Column Family），但 TinyKV 通过在键前面加上列族前缀的方式来模拟这个功能。

**事务集成**： TinyKV 利用 Badger 的事务功能来保证操作的原子性，这对于实现上层的分布式事务非常重要。



### **Part A：单机存储引擎**

这部分要实现 `StandaloneStorage` 结构体，主要包括几个核心方法：

- `NewStandaloneStorage()` 用来创建存储实例
- `Start()` 和 `Stop()` 管理存储的生命周期
- `Reader()` 和 `Write()` 提供读写接口

单机存储本身不复杂，主要是要理解 TinyKV 的存储抽象。底层使用的是 BadgerDB，这是一个类似 RocksDB 的嵌入式键值数据库。我们需要把 BadgerDB 的接口包装成 TinyKV 需要的格式。

这里可以参考TiKV的实现： raftStore将处理所有提交日志，并将其复制到不同组中的不同节点，这个组称为 Region

在引导阶段之后先只有一个Region，后续这个Region可能会被拆分成更多的Region， 不同的Region负责不同的key范围，多Region独立处理客户端请求。 

在lab1中为了简化任务，首先考虑的是单机（standalone）的存储引擎，后续再设置raft接 口啥的，这就是lab1P0的任务

[StandAloneStorage.go]()

的常用方法及其作用如下： 

- Start 方法：启动存储引擎。在单节点存储中，不需要与调度器客户端交互，因此直接返回nil 
- Stop方法：停止存储引擎，关闭Badger数据库 
- Reader 方法：创建并返回一个BadgerReader实例，用于读取数据
- Write方法：处理写操作

其中**reader**和**write**是我们需要写的

reader：因为要创建一个BadgerReader实例，所以首先查看BadgerReader的构造函数，就在该文件下：

```go
// NewBadgerReader 创建一个新的 BadgerReader 实例
// txn: 用于读取的 Badger 事务
// 返回: 初始化的 BadgerReader 实例
func NewBadgerReader(txn *badger.Txn) *BadgerReader {
	return &BadgerReader{txn}
}
```

可以看到，需要传入一个事务作为参数，所以接下来应该看如何创建新事务，首 先查看该文件，没有相关方法。因为是 badger.Txn ,猜测和badger有关，所以查 看文件开头导入的第一个包

```go
import (
	"github.com/Connor1996/badger"
	"github.com/pingcap-incubator/tinykv/kv/config"
	"github.com/pingcap-incubator/tinykv/kv/raftstore/scheduler_client"
	"github.com/pingcap-incubator/tinykv/kv/storage"
	"github.com/pingcap-incubator/tinykv/kv/util/engine_util"
	"github.com/pingcap-incubator/tinykv/proto/pkg/kvrpcpb"
)
```

根据提示，找到NewTransaction

```go
// Reader 创建一个存储读取器，用于读取数据
// ctx: RPC 上下文，包含事务信息等
// 返回: StorageReader 接口实例和可能的错误
// 需要实现：创建只读事务并返回 BadgerReader
func (s *StandAloneStorage) Reader(ctx *kvrpcpb.Context) (storage.StorageReader, error) {
	// YOUR CODE HERE (lab1).
	// 提示：
	// 1. 使用 s.db.NewTransaction(false) 创建只读事务
	// 2. 用 NewBadgerReader 包装事务并返回
	txn := s.db.NewTransaction(false)
	reader := NewBadgerReader(txn)
	return reader, nil
}
```

write：首先查看传入的两个参数：ctx表示上下文对象，用于存储一些请求的元数据；batch 表示一组修改操作，类型为[]storage.Modify 。

**storage.Modify** 是一个结构体，包含一个Data字段，访问该字段可以获取具体的操作类型和相关的数据。修改操作可以分为两个类型，应该对这两种操作分别进行处理： 

- Put操作：将数据插入到数据库中 
- Delete 操作：从数据库中删除数据

根据提示完成代码。我们写了详细的注释

```go
// Write 执行批量写操作
// 这是存储引擎的核心写入方法，负责：
// 1. 创建数据库事务来保证操作的原子性
// 2. 处理批量的 Put 和 Delete 操作
// 3. 将所有修改提交到底层存储
//
// 参数说明：
//   ctx: RPC 上下文信息，包含请求的元数据
//   batch: 要执行的修改操作列表，可能包含 Put 和 Delete 操作
// 返回值：
//   error: 写入过程中的错误，如果有的话
func (s *StandAloneStorage) Write(ctx *kvrpcpb.Context, batch []storage.Modify) error {
    // YOUR CODE HERE (lab1).
    // 提示：
    // 1. 检查 storage.Modify 的定义，了解 Put 和 Delete 操作
    // 2. 使用 badger 的事务接口执行批量操作
    // 3. 由于 badger 不直接支持列族，使用包装器来模拟
    // 4. 使用 engine_util.PutCF 和 engine_util.DeleteCF 方法

    // === 第一步：创建数据库事务 ===
    // 创建可写事务（参数 true 表示可写）
    // 事务确保所有操作要么全部成功，要么全部失败
    writeTxn := s.db.NewTransaction(true) 
    defer writeTxn.Discard() // 确保事务资源得到释放

    // === 第二步：处理批量修改操作 ===
    // 遍历所有的修改操作，支持 Put 和 Delete 两种类型
    for _, modification := range batch { 
        // 使用类型断言判断操作类型并执行相应的操作
        switch modification.Data.(type) {
        case storage.Put:
            // 处理 Put 操作：将键值对写入指定列族
            putOperation := modification.Data.(storage.Put)
            
            // 使用 KeyWithCF 将列族信息编码到键中
            // 这是因为 Badger 原生不支持列族，需要通过键前缀模拟
            operationError := writeTxn.Set(
                engine_util.KeyWithCF(putOperation.Cf, putOperation.Key), 
                putOperation.Value,
            )
            if operationError != nil {
                return operationError
            }
            
        case storage.Delete:
            // 处理 Delete 操作：从指定列族中删除键
            deleteOperation := modification.Data.(storage.Delete)
            
            // 同样使用 KeyWithCF 来处理列族信息
            operationError := writeTxn.Delete(
                engine_util.KeyWithCF(deleteOperation.Cf, deleteOperation.Key),
            )
            if operationError != nil {
                return operationError
            }
        }
    }

    // === 第三步：提交事务 ===
    // 将所有修改原子性地提交到存储引擎
    // 如果提交失败，所有操作都会被回滚
    if commitError := writeTxn.Commit(); commitError != nil {
        return commitError
    }

    return nil
}
```

### PART B：raftstore功能补全(P1-P4)

![image-20250609222721829](C:\Users\tel13803506661\AppData\Roaming\Typora\typora-user-images\image-20250609222721829.png)

如图，前面我们已经提到了raftstore的详解，在这就不赘述

那我们直接开始讲解代码，首先是 peer_Msg_handler.go 中的proposeRaftCommand 方法

```go
// proposeRaftCommand 处理客户端的 Raft 命令提议请求
// 这是分布式共识的入口方法，负责：
// 1. 验证命令的合法性（权限、任期、区域等检查）
// 2. 确保节点状态正常（未停止、能够处理请求）
// 3. 将命令提议给 Raft 组进行共识处理
//
// 在分布式系统中，只有通过 Raft 共识的命令才能被执行，
// 这保证了数据的一致性和系统的可靠性。
//
// 参数说明：
//   msg: 客户端发送的 Raft 命令请求
//   cb: 回调函数，用于返回处理结果给客户端
func (d *peerMsgHandler) proposeRaftCommand(msg *raft_cmdpb.RaftCmdRequest, cb *message.Callback) {
    // YOUR CODE HERE (lab1).
    
    // === 第一步：命令预检查 ===
    // Hint1: do `preProposeRaftCommand` check for the command, if the check fails, need to execute the
    // callback function and return the error results. `ErrResp` is useful to generate error response.
    
    // 执行命令的前置验证，包括：
    // - 存储 ID 检查：确保消息发送到正确的存储节点
    // - 领导者检查：只有领导者才能处理写请求
    // - 节点 ID 检查：确保消息发送到正确的节点
    // - 任期检查：防止处理过期的请求
    // - 区域纪元检查：确保区域信息是最新的
    commandCheckError := d.preProposeRaftCommand(msg) 
    if commandCheckError != nil {
        // 如果任何检查失败，立即通过回调返回错误响应
        log.Warn(fmt.Sprintf("[region %d] Command validation failed: %v", d.regionId, commandCheckError))
        cb.Done(ErrResp(commandCheckError))
        return
    }

    // === 第二步：节点状态检查 ===
    // Hint2: Check if peer is stopped already, if so notify the callback that the region is removed, check
    // the `destroy` function for related utilities. `NotifyReqRegionRemoved` is useful to generate error response.

    // 检查当前节点是否已经停止服务
    // 如果节点已停止，需要清理资源并通知客户端区域已被移除
    if d.stopped {
        log.Warn(fmt.Sprintf("[region %d] Node has been stopped, cannot process requests", d.regionId))
        
        // 尝试清理节点资源
        destroyError := d.Destroy(d.ctx.engine, false) 
        if destroyError != nil {
            log.Error(fmt.Sprintf("[region %d] Failed to destroy stopped node: %v", d.regionId, destroyError))
        }
        
        // 通知客户端该区域已被移除，无法继续处理请求
        NotifyReqRegionRemoved(d.regionId, cb)
        return
    }

    // === 第三步：准备响应并提交提议 ===
    // Hint3: Bind the possible response with term then do the real requests propose using the `Propose` function.
    // 提议请求时，需要将当前的 Raft term（任期）信息绑定到响应中
    // Note:
    // The peer that is being checked is a leader. It might step down to be a follower later. It
    // doesn't matter whether the peer is a leader or not. If it's not a leader, the proposing
    // command log entry can't be committed. There are some useful information in the `ctx` of the `peerMsgHandler`.

    // 创建命令响应对象并绑定当前任期
    // 任期信息用于客户端检测领导者变更和处理网络分区场景
    commandResponse := newCmdResp()
    BindRespTerm(commandResponse, d.Term())

    // 将命令提议给 Raft 组进行共识处理
    // Propose 方法会：
    // 1. 将命令添加到 Raft 日志
    // 2. 尝试与其他节点达成共识
    // 3. 在命令被提交后执行回调
    proposalSuccessful := d.peer.Propose(d.ctx.engine.Raft, d.ctx.cfg, cb, msg, commandResponse)
    
    if !proposalSuccessful {
        // 提议失败可能的原因：
        // - 节点不再是领导者
        // - Raft 组状态异常
        // - 系统资源不足
        log.Warn(fmt.Sprintf("[region %d] Failed to propose command to Raft group", d.regionId))
        return
    }

    // 提议成功提交到 Raft，等待共识完成
    log.Debug(fmt.Sprintf("[region %d] Command successfully proposed to Raft group", d.regionId))
}
```

proposeRaftCommand 接受两个参数，一个是要提议的 Raft 命令请求，另一个是回调函数，用于处理命令执行结果。函数中有三个提示，可以跟着提示一步步填充代码。代码中写了详细的注释，由于篇幅原因此处不再赘述

kv/raftstore/peer.go文件中，该文件主要定义了Raft节点（Peer）的相关结构体和方法，用于处理客户端请求、管理Raft状态、与其他节点通信等。 需要填充的是HandleRaftReady方法，该方法处理了Raft的Ready状态（包含待提交日志、快照、消息等），并执行相应的更新操作。

该方法接受三个参数：

- msgs表示消息队列；
- pdScheduler表示调度器通道，用于发送任务；
- trans表示传输层，用于发送消息。返回值分别表示快照应用结果和消息队列。

```go
func (p *peer) HandleRaftReady(msgs []message.Msg, pdScheduler chan<- worker.Task, trans Transport) (*ApplySnapResult, []message.Msg) {

	if p.stopped {
		return nil, msgs
	}

	// 如果有快照但尚未准备好，则记录日志并返回，等待下一次处理
	if p.HasPendingSnapshot() && !p.ReadyToHandlePendingSnap() {
		log.Debug(fmt.Sprintf("%v [apply_id: %v, last_applying_idx: %v] is not ready to apply snapshot.", p.Tag, p.peerStorage.AppliedIndex(), p.LastApplyingIdx))
		return nil, msgs
	}

	// YOUR CODE HERE (lab1). There are some missing code pars marked with `Hint` above, try to finish them.
	// Hint1: check if there's ready to be processed, if no return directly.
	// panic("not implemented yet")
	if !p.RaftGroup.HasReady() { // tinykv/raft/rawnode.go
		log.Debug(fmt.Sprintf("%v no raft ready", p.Tag))
		return nil, msgs
	}

	// 开始处理 ready 状态
	// Start to handle the raft ready.
	log.Debug(fmt.Sprintf("%v handle raft ready", p.Tag))

	ready := p.RaftGroup.Ready()
	// TODO: workaround for:
	//   in kvproto/eraftpb, we use *SnapshotMetadata
	//   but in etcd, they use SnapshotMetadata
	// 如果 Ready 中包含快照，但元数据为空，则初始化元数据（兼容性处理）
	if ready.Snapshot.GetMetadata() == nil {
		ready.Snapshot.Metadata = &eraftpb.SnapshotMetadata{}
	}

	// The leader can write to disk and replicate to the followers concurrently
	// For more details, check raft thesis 10.2.1.
	// 如果当前节点是领导者（IsLeader 返回 true）：
	if p.IsLeader() {
		p.Send(trans, ready.Messages)       // 将 Ready 中的消息通过传输层（trans）发送给其他节点。
		ready.Messages = ready.Messages[:0] // 清空消息，避免重复发送
	}

	// 处理软状态（如果ready中包含）

	// 软状态（Soft State） 是一种 非持久化 的、在内存中维护的运行时状态信息。
	// 与硬状态（Hard State）相比，软状态不需要被持久化到存储中，因为它可以在节点重启后通过 Raft 协议重新推导或恢复
	// 软状态的主要内容：1.节点角色（RaftState）2.领导者信息（LeaderID）3.选举超时计时器（ElectionElapsed）4.心跳计时器（HeartbeatElapsed）

	// 为什么在软状态变化时发送心跳？
	// 软状态的变化可能表示以下情况：
	// 1.当前节点刚刚成为领导者（RaftState == Leader）
	// 2.需要通知其他节点当前的领导者是谁，以及维持领导者状态
	ss := ready.SoftState
	if ss != nil && ss.RaftState == raft.StateLeader {
		p.HeartbeatScheduler(pdScheduler) // 发送心跳任务，通知其他节点该节点仍然是领导者
	}

	// 持久化日志条目和快照，防止数据丢失，更新硬状态，确保系统崩溃后可以正确恢复
	applySnapResult, err := p.peerStorage.SaveReadyState(&ready)
	if err != nil {
		panic(fmt.Sprintf("failed to handle raft ready, error: %v", err))
	}
	if !p.IsLeader() { // 如果当前节点不是领导者，再次发送消息，确保日志或快照同步
		p.Send(trans, ready.Messages)
	}

	// 处理快照和日志条目，确保状态机的正确更新，优先处理快照
	// 如果快照应用成功（applySnapResult != nil），则生成一个刷新消息并加入消息队列，并更新快照的应用索引
	if applySnapResult != nil {
		/// Register self to applyMsgs so that the peer is then usable.
		msgs = append(msgs, message.NewPeerMsg(message.MsgTypeApplyRefresh, p.regionId, &MsgApplyRefresh{
			id:     p.PeerId(),
			term:   p.Term(),
			region: p.Region(),
		}))

		// Snapshot's metadata has been applied.
		// 快照的元数据已经被应用，更新应用索引
		p.LastApplyingIdx = p.peerStorage.truncatedIndex()
	} else { // 没有新的快照应用，则处理 Ready 中的已提交日志条目
		committedEntries := ready.CommittedEntries
		ready.CommittedEntries = nil
		l := len(committedEntries)
		if l > 0 {
			p.LastApplyingIdx = committedEntries[l-1].Index
			// 通知状态机执行提交的日志条目
			msgs = append(msgs, message.Msg{Type: message.MsgTypeApplyCommitted, Data: &MsgApplyCommitted{
				regionId: p.regionId,
				term:     p.Term(),
				entries:  committedEntries,
			}, RegionID: p.regionId})
		}
	}

	// YOUR CODE HERE (lab1). There are some missing code pars marked with `Hint` above, try to finish them.
	// Hint2: Try to advance the states in the raft group of this peer after processing the raft ready.
	//        Check about the `Advance` method in for the raft group.
	// 推进 Raft 的状态，使其知道之前的 Ready 状态已经被处理，进入下一步
	p.RaftGroup.Advance(ready)

	return applySnapResult, msgs

}
```

**Ready** 是一个状态集合，用于描述当前节点需要处理的内容，包含以下几种可能需要执行的操作： 

- 消息：需要发送给其他节点的Raft消息（如心跳、日志复制）

- 日志条目：需要写入存储或提交给状态机的日志条目

- 快照：需要应用的快照，用于恢复或同步节点的状态

- 软状态：

  1.节点角色

  2.领导者信息

  3.选举超时计时器

  4.心跳计时器

- 硬状态：需要持久化的Raft状态信息（如当前任期、投票信息、提交索 引），硬状态用于在节点重启或故障恢复后，恢复 Raft 的关键状态信息。

该方法有两处需要填写，同样根据提示进行填写。代码中有详细注释阐述编写过程，在此处不再赘述。

 **kv/raftstore/peer_storage.go** 

该文件是TinyKV中一个关键组件，负责持久化Raft节点的状态和日志信息。它 实现了raft.Storage 接口，为 Raft 协议提供持久化的存储支持。此外，它还管理节 点的快照、日志条目，以及与节点生命周期相关的元数据。 在该文件中需要填充两个方法，分别是Append和SaveReadyState

 **Append** 该方法负责将给定的Raft日志条目追加到存储中，并更新节点的Raft状态，包括最新日志的索引和任期。同时，它处理新旧日志之间的冲突，删除不一致的旧日志，确保Raft的日志一致性。 传入的参数有两个，一个是日志条目数组，表示需要追加到Raft日志存储中的新日志条目；一个是批量写操作的工具，提供了高效地将多个键值对一次性写入存储引擎的方法。
 **SaveReadyState** 处理由 Raft 实例生成的 ready对象，主要任务是发送Raft消息给其他peers，并持久化日志条目。Raft日志条目将保存到Raft存储中，快照将应用到快照应用任务中，由region worker处理。将内存状态保存到磁盘。不要在此函数中修改ready对象，这是为了确保稍后正确推进ready对象。Raft ready的输出包括：快照、日志条目、状态，尝试正确处理它们。

注意，应用快照可能需要使用KV引擎，而其他操作将始终使用Raft引擎。观察已给出的代码，可以发现代码主要分成三部分：

1. 如果Ready包含快照，则应用快照
2. 如果Ready包含新的日志条目，将其持久化
3. 如果硬状态不为空，持久化硬状态
  其中快照处理部分已给出，我们需要完成接下来两部分。

**根据提示，填写代码**

代码中有详细注释讲解思路过程

```go
// Append 将指定的日志条目追加到 Raft 日志中
// 这是 Raft 日志管理的核心方法，负责：
// 1. 持久化新的日志条目到 Raft 存储引擎
// 2. 清理与新日志产生冲突的历史条目
// 3. 更新本地 Raft 状态的索引和任期信息
//
// 在 Raft 协议中，当节点接收到新的日志条目时，必须确保日志的一致性。
// 如果新日志与现有日志存在冲突（相同索引但不同内容），则需要删除冲突的条目。
//
// 参数说明：
//   entries: 需要追加的日志条目数组
//   raftWB: Raft 引擎的批量写入对象，用于批量操作
// 返回值：
//   error: 处理过程中遇到的错误
func (ps *PeerStorage) Append(entries []eraftpb.Entry, raftWB *engine_util.WriteBatch) error {
    log.Debug(fmt.Sprintf("%s append %d entries", ps.Tag, len(entries)))

    // 记录当前的最后日志索引，用于后续的冲突检测
    originalLastIndex := ps.raftState.LastIndex 

    // === 步骤一：输入合法性检查 ===
    // 如果传入的日志条目列表为空，无需进行任何处理
    if len(entries) == 0 {
        return nil
    }

    // === 步骤二：获取新日志的关键信息 ===
    // 获取要追加的日志条目中的最后一个条目
    finalLogEntry := entries[len(entries)-1] 
    lastIndex := finalLogEntry.Index
    lastTerm := finalLogEntry.Term

    // YOUR CODE HERE (lab1).
    // === 步骤三：持久化所有新的日志条目 ===
    // 遍历每个日志条目，将其写入到 Raft 存储引擎中
    for _, entry := range entries {
        // Hint1: in the raft write batch, the log key could be generated by `meta.RaftLogKey`.
        //       Also the `LastIndex` and `LastTerm` raft states should be updated after the `Append`.
        //       Use the input `raftWB` to save the append results, do check if the input `entries` are empty.
        //       Note the raft logs are stored as the `meta` type key-value pairs, so the `RaftLogKey` and `SetMeta`
        //       functions could be useful.
        // 提示1：在 Raft 写批次中，日志键可以通过 `meta.RaftLogKey` 生成。
        //       同时，`LastIndex` 和 `LastTerm` Raft 状态应该在追加日志条目后更新。
        //       使用输入的 `raftWB` 保存追加结果，检查输入的 `entries` 是否为空。
        //       注意，Raft 日志存储为 `meta` 类型的键值对，因此 `RaftLogKey` 和 `SetMeta` 函数可能会有用。

        // 为每个日志条目生成在存储中的唯一标识键
        // 键的格式包含 region ID 和日志索引，确保全局唯一性
        key := meta.RaftLogKey(ps.region.GetId(), entry.GetIndex())
        
        // 将日志条目序列化并添加到批量写入操作中
        // SetMeta 方法会处理序列化和存储的细节
        raftWB.SetMeta(key, &entry)
        
        log.Debug(fmt.Sprintf("Prepared to persist log entry: index=%d, term=%d", 
            entry.GetIndex(), entry.GetTerm()))
    }

    // === 步骤四：清理冲突的历史日志条目 ===
    // 根据 Raft 协议，当接收到新的日志条目时，需要删除所有索引大于新日志的旧条目
    // 这确保了日志的一致性：新的条目会覆盖可能存在冲突的旧条目
    for i := lastIndex + 1; i <= originalLastIndex; i++ {
        // Hint2: As the to be append logs may conflict with the old ones, try to delete the left
        //       old ones whose entry indexes are greater than the last to be append entry.
        //       Delete these previously appended log entries which will never be committed.
        // 提示2：由于要追加的日志条目可能与旧的日志条目冲突，尝试删除索引大于最后一个要追加的日志条目的旧日志条目。
        //       删除这些之前追加的但永远不会被提交的日志条目。
        
        // 生成要删除的冲突日志条目的存储键
        key := meta.RaftLogKey(ps.region.GetId(), i)
        
        // 将删除操作添加到批量写入中
        raftWB.DeleteMeta(key)
        
        log.Debug(fmt.Sprintf("Marked conflicting log entry for deletion: index=%d", i))
    }

    // === 步骤五：更新本地 Raft 状态 ===
    // 更新当前节点维护的 Raft 状态信息
    // 这些状态会在后续的 SaveReadyState 中持久化到存储
    ps.raftState.LastIndex = lastIndex  // 更新最后日志索引
    ps.raftState.LastTerm = lastTerm    // 更新最后日志任期
    
    log.Debug(fmt.Sprintf("Updated local raft state: LastIndex=%d, LastTerm=%d", 
        lastIndex, lastTerm))

    return nil
}
```

```go
// SaveReadyState 处理由 Raft 实例生成的 Ready 状态
// 这是 Raft 持久化的核心方法，主要任务包括：
// 1. 处理快照应用（如果有）
// 2. 持久化新的日志条目
// 3. 更新并持久化硬状态（HardState）
// 4. 将所有更改提交到存储引擎
//
// 重要说明：不要在此函数中修改 ready 对象，这是后续正确推进 ready 对象的要求
//
// 参数：
//   ready: 包含需要持久化的状态变更的 Ready 对象
// 返回：
//   *ApplySnapResult: 快照应用结果（如果应用了快照）
//   error: 处理过程中的错误
func (ps *PeerStorage) SaveReadyState(ready *raft.Ready) (*ApplySnapResult, error) {
    // === 第一步：初始化批量写入对象 ===
    // kvWB 用于状态机相关的写入（快照数据、应用状态等）
    // raftWB 用于 Raft 日志相关的写入（日志条目、Raft 状态等）
    kvWB, raftWB := new(engine_util.WriteBatch), new(engine_util.WriteBatch)

    // 保存当前 Raft 状态的副本，用于后续比较是否发生变化
    // 只有状态确实改变时才需要持久化，避免不必要的写入
    currentRaftState := ps.raftState 
    var snapshotResult *ApplySnapResult = nil 
    var processingError error

    // === 第二步：处理快照应用（如果存在）===
    // 快照应用的优先级最高，因为它会重置整个状态机
    if !raft.IsEmptySnap(&ready.Snapshot) {
        log.Debug(fmt.Sprintf("%s applying snapshot with index=%d", 
            ps.Tag, ready.Snapshot.Metadata.Index))
        
        snapshotResult, processingError = ps.ApplySnapshot(&ready.Snapshot, kvWB, raftWB)
        if processingError != nil {
            return nil, fmt.Errorf("failed to apply snapshot: %w", processingError)
        }
    }

    // YOUR CODE HERE (lab1).
    // Hint: the outputs of the raft ready are: snapshot, entries, states, try to process
    //       them correctly. Note the snapshot apply may need the kv engine while others will
    //       always use the raft engine.
    // 提示：Raft ready 的输出包括：快照、日志条目、状态，尝试正确处理它们。
    //       注意，应用快照可能需要使用 KV 引擎，而其他操作将始终使用 Raft 引擎。

    // === 第三步：处理日志条目持久化 ===
    if len(ready.Entries) != 0 {
        // Hint1: Process entries if it's not empty.
        // 如果日志条目不为空，处理日志条目。
        
        log.Debug(fmt.Sprintf("%s persisting %d new log entries", 
            ps.Tag, len(ready.Entries)))
        
        // 调用 Append 方法将新的日志条目持久化到 Raft 日志存储
        // 这包括：将条目写入存储 + 清理冲突的旧条目 + 更新本地状态
        appendError := ps.Append(ready.Entries, raftWB)
        if appendError != nil {
            return nil, fmt.Errorf("failed to append log entries: %w", appendError)
        }
    }

    // === 第四步：处理硬状态更新 ===
    // LastIndex 为 0 意味着该 peer 是从 Raft 消息创建的
    // 并且尚未应用快照，因此跳过硬状态的持久化
    if ps.raftState.LastIndex > 0 {
        // Hint2: Handle the hard state if it is NOT empty.
        // 提示2：如果硬状态不为空，处理硬状态。
        
        // 检查 Ready 中是否包含需要持久化的硬状态
        // 硬状态包括：当前任期(Term)、投票对象(Vote)、提交索引(Commit)
        if !raft.IsEmptyHardState(ready.HardState) {
            log.Debug(fmt.Sprintf("%s updating hard state: term=%d, vote=%d, commit=%d", 
                ps.Tag, ready.HardState.Term, ready.HardState.Vote, ready.HardState.Commit))
            
            // 更新本地的硬状态
            ps.raftState.HardState = &ready.HardState
        }
    }

    // === 第五步：条件性持久化 Raft 状态 ===
    // 只有当 Raft 状态确实发生变化时才进行持久化
    // 这是一个重要的性能优化：避免不必要的磁盘写入
    if !proto.Equal(&currentRaftState, &ps.raftState) {
        log.Debug(fmt.Sprintf("%s persisting updated raft state", ps.Tag))
        
        // 将更新后的 Raft 状态添加到写批次中
        // RaftStateKey 生成该 region 对应的 Raft 状态存储键
        raftWB.SetMeta(meta.RaftStateKey(ps.region.GetId()), &ps.raftState)
    }

    // === 第六步：原子性提交所有更改 ===
    // 使用 MustWriteToDB 确保所有更改都成功写入存储
    // 如果写入失败，程序会 panic，确保数据一致性
    
    // 提交状态机相关的更改（快照数据、应用状态等）
    kvWB.MustWriteToDB(ps.Engines.Kv)
    
    // 提交 Raft 日志相关的更改（日志条目、Raft 状态等）
    raftWB.MustWriteToDB(ps.Engines.Raft)

    log.Debug(fmt.Sprintf("%s successfully saved ready state", ps.Tag))
    
    return snapshotResult, nil
}
```

### 测试截图

![lab1p1a](D:\桌面\大二\数据库\img\lab1p1a.png)![lab1P0](D:\桌面\大二\数据库\img\lab1P0.png)

![lab1p1b](D:\桌面\大二\数据库\img\lab1p1b.png)

![lab1p2a](D:\桌面\大二\数据库\img\lab1p2a.png)

![lab1p2b](D:\桌面\大二\数据库\img\lab1p2b.png)

![lab1p3a](D:\桌面\大二\数据库\img\lab1p3a.png)

![lab1p3b](D:\桌面\大二\数据库\img\lab1p3b.png)

![lab1p4a](D:\桌面\大二\数据库\img\lab1p4a.png)

![lab1p4b](D:\桌面\大二\数据库\img\lab1p4b.png)

# LAB 2 The Transaction Layer - TinyKV Part

![image-20250609224147338](C:\Users\tel13803506661\AppData\Roaming\Typora\typora-user-images\image-20250609224147338.png)

Lab1 我们已经搭建好了 Raft 日志引擎和存储引擎的基础。Raft 日志引擎保证了事务日志能够可靠地持久化存储，即使发生故障也能恢复到正确的状态。现在在 Lab2 中，我们要在这个基础上实现分布式事务层。

如果说 Raft 解决了数据一致性问题，那么事务层就是要解决并发访问时的原子性和隔离性问题。多个事务同时执行时，我们需要确保每个事务要么完全成功，要么完全失败，不能出现部分成功的情况。

### Percolator 协议

TinyKV 使用 Percolator 协议来实现分布式事务。这个协议最初是 Google 为了处理大规模数据更新而设计的，后来被 TiDB 采用并进行了优化。

**核心思想**：

- 使用两阶段提交来保证事务的原子性
- 通过全局时间戳来确定事务的执行顺序
- 基于 MVCC（多版本并发控制）来实现事务隔离

**时间戳机制**： 每个事务都会分配两个时间戳：

- **start_ts**：事务开始时分配，用于读取数据的快照版本
- **commit_ts**：事务提交时分配，用于标记写入数据的版本

这些时间戳都是由 TinyScheduler 统一分配的，保证全局单调递增。

**示例流程**

```
假设有事务A和B，同步访问键K事务。A获取时间戳 StartTS = 10，开始修改K事务B获取时间戳 StartTS = 15，也尝试修改K。A加锁成功，完成Prewrite阶段B检测到K被锁定，阻塞等待A进入Commit阶段，分配CommitTS = 20，提交成功B继续尝试，但发现K的CommitTS = 20，大于B的StartTS = 15，回滚自身事务
```

**tinykv/kv/transaction/commands/command.go**

该文件定义了一个抽象的事务命令接口 Command 以及一个运行事务命令的函数 RunCommand。这个文件的主要作用是为 TinyKV 项目中的事务处理提供统一的接口和执行流程，避免在具体命令实现中出现重复代码。

WillWrite 方法用于确定该请求是否需要执行写入操作，Read 方法负责执行命令所需的读取操作。PrepareWrites 方法是写入命令处理的核心部分，用于构建该命令的具体写入内容。由于每个事务都有其唯一的标识符，也就是分配的全局时间戳 start_ts，因此 StartTs 方法用于返回当前命令对应的时间戳值。

 **tinykv/kv/transaction/commands/get.go**

该文件定义了一个Get命令，用于从数据库中读取数据。这个文件的主要作用是 实现一个只读的事务命令，处理客户端的Get请求，并从数据库中检索相应的数据。

我们需要完成其中的read函数

```go
// 从数据库中读取数据
func (g *Get) Read(txn *mvcc.RoTxn) (interface{}, [][]byte, error) {
	// 从 Get 命令的请求数据中获取 key，tinykv/proto/pkg/kvrpcpb/kvrpcpb.pb.go
	key := g.request.Key

	// 记录日志，包括事务的开始时间戳和要读取的键
	log.Debug("read key", zap.Uint64("start_ts", txn.StartTS),
		zap.String("key", hex.EncodeToString(key)))

	// 创建一个新的 kvrpcpb.GetResponse 类型的指针，用于存储 Get 请求的响应
	// tinykv/proto/pkg/kvrpcpb/kvrpcpb.pb.go
	response := new(kvrpcpb.GetResponse)

	// panic("kv get is not implemented yet")
	// YOUR CODE HERE (lab2).
	// Check for locks and their visibilities.
	// Hint: Check the interfaces provided by `mvcc.RoTxn`.
	lock, err := txn.GetLock(key) // tinykv/kv/transaction/mvcc/transaction.go
	if err != nil {
		return nil, nil, err
	}
	// 存在锁且指定的键在给定的事务开始时间戳被锁定
	// if lock != nil && lock.Ts < txn.StartTS {	// tinykv/kv/transaction/mvcc/lock.go
	if lock != nil && lock.IsLockedFor(key, g.startTs, response) {
		// 错误类型是一个 KeyError，说明当前键的读取操作因锁冲突而失败
		// kvrpcpb.GetResponse 的 Error 类型是 *KeyError（type KeyError struct定义）
		response.Error = &kvrpcpb.KeyError{
			Locked: lock.Info(key),
		}
		return response, nil, nil
	}

	// YOUR CODE HERE (lab2).
	// Search writes for a committed value, set results in the response.
	// Hint: Check the interfaces provided by `mvcc.RoTxn`.
	// tinykv/kv/transaction/mvcc/transaction.go
	value, err := txn.GetValue(key)
	if err != nil {
		return nil, nil, err
	}
	if value == nil {
		response.NotFound = true
	} else {
		response.Value = value
	}

	return response, nil, nil

}
```

首先从Get请求中提取目标数据的键，检查目标键是否存在锁，如果键没有被锁定，则查找存储引擎中该键的已提交值，最后返回响应结构体。需要我们填充的 是检查锁和获取数据两部分，我们根据提示进行填写

 **tinykv/kv/transaction/commands/prewrite.go**

该文件定义了 Prewrite 命令，用于处理事务的预写阶段。预写是两阶段提交协议中的第一个阶段，包含了事务中的所有写操作（读操作不在此阶段处理）。在这个阶段，系统会检查事务是否能够原子地写入底层存储，同时确保不会与其他事务（无论是已完成的还是正在进行的）发生冲突。如果检查通过，就向客户端返回成功响应。只有当客户端收到所有 key 的预写成功响应后，才会发送 commit 消息进入第二阶段。

我们要填写的是prewriteMutation 方法，用于处理每一个变更。在写入锁和键之前需要检查写入冲突和锁状态。

```go
// prewriteMutation 执行单个变更操作的预写处理
// 这是两阶段提交协议第一阶段的核心实现，负责：
// 1. 检查写写冲突：确保没有其他事务在当前事务开始后修改了同一个键
// 2. 检查锁冲突：确保目标键没有被其他事务锁定
// 3. 创建预写锁：为当前操作创建锁，防止其他事务并发修改
// 4. 写入数据：将新值写入 MVCC 存储（如果是 Put 操作）
//
// 返回值说明：
//   - (nil, nil): 预写成功
//   - (keyError, nil): 键相关错误（锁冲突或写冲突）
//   - (nil, systemError): 系统内部错误
//
// 参数：
//   txn: MVCC 写事务，提供写入和冲突检测能力
//   mut: 要执行的变更操作（Put 或 Delete）
func (p *Prewrite) prewriteMutation(txn *mvcc.MvccTxn, mut *kvrpcpb.Mutation) (*kvrpcpb.KeyError, error) {
    mutationKey := mut.Key 

    log.Debug("prewrite key", zap.Uint64("start_ts", txn.StartTS),
        zap.String("key", hex.EncodeToString(mutationKey)))

    // YOUR CODE HERE (lab2).
    // Check for write conflicts.
    // Hint: Check the interafaces provided by `mvcc.MvccTxn`. The error type `kvrpcpb.WriteConflict` is used
    //		 denote to write conflict error, try to set error information properly in the `kvrpcpb.KeyError`
    //		 response.
    
    // === 第一步：检查写写冲突 ===
    // 写写冲突检测是保证 Snapshot Isolation 的关键
    // 如果目标键在当前事务开始后被其他事务修改，则存在冲突
    latestWrite, conflictTimestamp, writeError := txn.MostRecentWrite(mutationKey)
    if writeError != nil {
        return nil, writeError
    }
    
    // 检查是否存在写写冲突
    // 冲突条件：最新提交版本的时间戳 >= 当前事务的开始时间戳
    if latestWrite != nil && conflictTimestamp >= txn.StartTS {
        // 构造写冲突错误响应
        conflictError := &kvrpcpb.KeyError{ 
            Conflict: &kvrpcpb.WriteConflict{
                StartTs:    txn.StartTS,           // 当前事务的开始时间戳
                ConflictTs: conflictTimestamp,     // 冲突的提交时间戳
                Key:        mutationKey,           // 冲突的键
                Primary:    p.request.PrimaryLock, // 当前事务的主键
            },
        }
        return conflictError, nil
    }

    // YOUR CODE HERE (lab2).
    // Check if key is locked. Report key is locked error if lock does exist, note the key could be locked
    // by this transaction already and the current prewrite request is stale.
    
    // === 第二步：检查锁冲突 ===
    // 锁冲突检测确保同一时刻只有一个事务能修改某个键
    existingLock, lockError := txn.GetLock(mutationKey)
    if lockError != nil {
        return nil, lockError
    }
    
    if existingLock != nil {
        // 检查锁的归属
        if existingLock.Ts != txn.StartTS {
            // 情况1：锁属于其他事务，返回锁冲突错误
            lockConflictError := &kvrpcpb.KeyError{
                Locked: existingLock.Info(mutationKey), // 返回锁的详细信息
            }
            return lockConflictError, nil
        } else {
            // 情况2：锁属于当前事务，可能是重复请求
            // 直接返回成功，避免重复操作
            return nil, nil
        }
    }

    // YOUR CODE HERE (lab2).
    // Write a lock and value.
    // Hint: Check the interfaces provided by `mvccTxn.Txn`.
    
    // === 第三步：创建预写锁和写入数据 ===
    // 在没有冲突的情况下，为当前操作创建锁并写入数据
    
    // 创建预写锁
    // 锁包含了事务的关键信息，用于后续的提交或回滚
    prewriteLock := &mvcc.Lock{
        Primary: p.request.PrimaryLock,            // 事务的主键（用于协调）
        Ts:      txn.StartTS,                     // 事务的开始时间戳
        Ttl:     p.request.LockTtl,               // 锁的生存时间
        Kind:    mvcc.WriteKindFromProto(mut.Op), // 操作类型（Put/Delete）
    }
    
    // 将锁写入存储
    txn.PutLock(mutationKey, prewriteLock)
    
    // 根据操作类型写入或删除数据
    switch mut.Op {
    case kvrpcpb.Op_Put:
        // Put 操作：写入新值到 MVCC 存储
        // 数据以 (key, start_ts) -> value 的形式存储
        txn.PutValue(mutationKey, mut.Value)
        
    case kvrpcpb.Op_Del:
        // Delete 操作：标记键为删除状态
        // 在 MVCC 中，删除是通过写入删除标记实现的
        txn.DeleteValue(mutationKey)
    }

    // 预写成功完成
    return nil, nil
}
```

根据提示填入代码即可。

**tinykv/kv/transaction/commands/rollback.go**

该文件定义了回滚命令及其相关方法，用于处理事务的回滚操作。事务的回滚是 为了撤销未提交的事务，确保数据库的一致性。它同样包含基础命令的接口和一 个回滚请求字段

和提交操作一样，该文件的主体由PrepareWrites和rollbackKey组成，前者用 于创建响应对象并遍历回滚请求中的所有键，后者则用于回滚单个键。需要填 写的是rollbackKey 中的部分内容。